

	                       Cheating Detection Software

	  	   		   Maya Rosenthal and Eleazar Eskin


I. Implementation Overview

	I have implemented the Smith-Waterman local alignment algorithm.  It
compares two sequences and finds the best-aligned subsequences between them.  It
does so in the following way.  First, a substitution matrix is created.  This
substitution matrix holds values for each possible pair of aligned elements of a
sequence.  Each pair is given a score that gives the relative likely hood that
the elements are related as opposed to unrelated.  Next a gap penalty must be
determined, to penalized gaps in sequences.  Finally, the dynamic programming
matrix is created for the Smith Waterman algorithm.  This matrix stores score and
sequence information for the maximally aligned subsequence. 
	
	The implementation consists of two parts.  First, the program Convert.java
is used to convert a file to the appropriate format.  Given a 
file called <filename>, Convert.java removes all comments and System.out.println 
statements, then outputs to a file called "<filename>.t" each token on a separate 
line.  Anything and everything is a token -- keywords, symbols, variables, etc.  
Second, these tokenized files are given to Detect.java, two at a time, to determine 
the similarity between them using the Smith-Waterman algorithm described above.  
The dynamic programming matrix is a 2 dimensional array.  Each element is determined by:
	F(i,j) = max { 0, F(i-1,j-1) + substitution matrix val, 
		          F(i-1, j) - gap penalty, F(i,j-1) - gap penalty }
In addition to keeping track of this score, each element of the matrix also keeps 
track of its predecessor.  For example, if F(i-1, j) - gap penalty is the maximum 
value and is assignment to F(i,j), then the predecessor is F(i-1,j).  Once the matrix
is completely filled, a search is search is done for the  maximum score (max{ F(i,j) }).  
By back-tracking the predecessors of the element with the maximum score, the program is 
able to find the subsequences with the greatest similarity.  If a similarity score of 
900 is reached, the two programs are flagged as potentially containing cheating.  

	 This algorithm gives no information about the type of cheating, but it
is sufficient to suggest cheating.   Take the following scenarios: a student
directly copied another student's homework and submitted it as his/her own, a
student copied only a method of another student, and a student copied several
different portions of another student's homework.  Regardless of how much
cheating there is, this algorithm will catch cheating within a certain threshold
(alignment score).  The key is to create a good substitution matrix and find a
good gap penalty.  These can be used to determine a good cheating threshold.  So
if all 3 scenarios have alignment scores equal to or greater than the threshold
(in our case, 900), then this algorithm is sufficient to suggest cheating.

II.  What works

	The files Convert.java, Detect.java, and MatrixNode.java, which
constitute the heart of the program and entire algorithm, all function well
and appear to be bug-free.  *See section V.  In addition, a slightly altered 
version of Detect.java called SummaryDetect.java is also included.  
boSummaryDetect.java accepts summary files, or file encodings, generated by a 
program created by Seno Park. 

III.  What doesn't work.

	Unfortunately, I do not know scripting and was not able to write a script that 
will take a directory and apply the cheating detection program to each file in that
directory, two at a time.
  
IV. Future

	Although, this project is not problem-free, the theory behind it is
strong.  I see this project as a first attempt to implement a theoretically solid
idea.  In the future, the problems I ran into can be fixed and the implementation
can be streamlined to create good cheating detection software.

	The biggest problem I ran into was the size of the adjacency matrix.  I
found that, even when I allocated the maximum amount of memory to the matrix, the
program still did not have enough memory to store the 2-dimentional array.  At
times I was able to sidestep this problem by only using every other token of a
program, but this is not a valid solution.  Clearly, this is a problem that needs
to be looked into.  Perhaps there is a more efficient way of implementing the
algorithm, maybe a 2-dimentional array can be avoided.  Perhaps, more information
can be eliminated during the tokenizing stage (for example, something like Seno
Park's encoding algorithm).  On one hand, summarizing the files will save
valuable space in memory, but on the other hand, a lot of information about the
student's programs will be lost.  Therefore, it is important to find a balance
between the two.

	I used an ad-hoc method to determine the values in the substitution 
matrix and the value of the gap penalty.  I tried to use what seemed
logical, for instance an exact match is +10 points (very likely), going from a
keyword to a symbol is -10 (very unlikely), while going from a variable to a
variable is +1 (likely).  Ideally, research would be done into these numbers to
find the values that give optimal results.  As I mentioned before, the values in
the substitution matrix and the value of the gap penalty are crucial to the
success of this algorithm.  To make the algorithm more efficient, these numbers
need to be examined more closely.

	This software is language-dependent.  The keywords and symbols in the
program are static arrays containing Java information.  An improvement would be
to either dynamically generate this information or to have flags turned on and
off specifying which language is used. 

	Speed is also an issue.  In my code I tried to leave out superfluous
lines and loops, to keep the runtime as quick as possible.  Currently, this
software takes about 15-20 seconds to run on two programs of approximately 300
lines.  Although the runtime is a bit slow, it is not the culprit -- the numbers
of comparisons needed are.  To compare 50 programs, it would take
50! Comparisons, about 3x10^64.  That, by itself is a very large number, but when
multiplied by the 20 seconds it takes to run each time, the number grows even
more.  Even if it takes a few days to run a class' homework through this program
it is acceptable.  Usually, homeworks are returned to students in about a week
and a few days (2 or 3) is plenty of time to test for cheating.  Minimizing the
time it takes to run this program is definitely important in the future, at the
very least it should be reduced to 2 or 3 days. 

	A possible addition to this software could be a database, to allow for
detecting cheating between current and past homeworks.  Again, the time issue
comes up.  To solve this, perhaps the database will not hold entire programs, but
methods labeled by what it does.  For example, we may want to check a program
with 3 methods against our database.  Assume that the first method reads in a
file, the next method sorts that file, the last method sends the output to a
file.  Instead of checking all possible combinations with the homeworks in the
database, it could find only the similar methods and compare those. 

	Finally, I would like to see research done into the algorithm
itself.  There are several different local alignment algorithms – ones that find
repeated matches, or overlapping matches, etc.  Research into how they compare
and how they differ will help improve on this software.
